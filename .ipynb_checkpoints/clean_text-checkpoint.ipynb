{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from pythainlp.util import normalize\n",
    "from pythainlp.ulmfit.utils import ThaiTokenizer\n",
    "from pythainlp.corpus import stopwords\n",
    "from pythainlp.tag import pos_tag\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, json, sys\n",
    "import copy, glob\n",
    "import pythainlp\n",
    "from tqdm import tqdm\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text_to_tokens\n",
    "- nomalize\n",
    "- remove duplicate ending character\n",
    "- remove url\n",
    "- remove cr\n",
    "- remove unwanted character\n",
    "- remove stopwords\n",
    "- to lowercase\n",
    "- tokenize\n",
    "- remove space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tokens(text, stop_words=[]):\n",
    "    # nomalize\n",
    "    text = pythainlp.util.normalize(text)\n",
    "    \n",
    "    # remove duplicate ending characters\n",
    "    for m in re.finditer(r'([\\u0E00-\\u0E7F])(\\1{2,})', text):\n",
    "        text = text.replace(m.group(0),m.group(1),1)\n",
    "        \n",
    "    # remove url\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # remove CR\n",
    "    text = text.replace('CR', '').replace('SR', '').replace('\\xa0', '')\n",
    "        \n",
    "    # remove unwanted character\n",
    "    pattern = re.compile(r\"[^\\u0E00-\\u0E7Fa-zA-Z ]|ๆ\")\n",
    "    text = pattern.sub('',text)\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # tokenization\n",
    "    tokens = pythainlp.tokenize.word_tokenize(text, engine='newmm', whitespaces=False)\n",
    "    \n",
    "    # remove stopwords\n",
    "    tokens = [t for t in tokens if not t in stop_words] \n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''สอบถามเพื่อเอาเเอพนี้ไปเป็นตัวเลือกในการทำการบ้านหน่อยค่ะ\n",
    "- เวลาเราโพสเเล้วเป็นนิรนาม เเล้วพอมีคนมาตอบเป็นเเอคนิรนามเหมือนกัน เราไม่สามารถเเยกเจ้าของโพสกับคนมาตอบได้เลยใช่มั้ย ถ้าในกรณีเเบบถ้าโดนเเอบอ้างมาเป็นคนโพส  \n",
    "- กดอุนจิ นี้คือไม่ชอบใช่มั้ย\n",
    "Thx'''\n",
    "text_to_tokens(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pos_filter_noun\n",
    "- filter noun from pos to find topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_filter_noun(tokens):\n",
    "    postags =  pos_tag(tokens, engine = 'artagger')\n",
    "    filterpos = []\n",
    "    noun = ['NPRP','NCMN']\n",
    "    for w in postags:\n",
    "        \n",
    "        # filter noun\n",
    "        if w[1] in noun:\n",
    "            filterpos.append(w[0])\n",
    "        \n",
    "    return filterpos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine_ques_type\n",
    "- find type of question sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_ques_type(text):\n",
    "    \n",
    "    # nomalize\n",
    "    text = normalize(text)\n",
    "    \n",
    "    # question keywords\n",
    "\n",
    "    where = ['ที่ไหน','อยู่ไหน','ตรงไหน','ไรดี'] \n",
    "    when = ['เมื่อไหร่','เปิดไหม','กี่โมง','กี่ทุ่ม','กี่นาที','กี่วัน','กี่เดือน','กี่ปี']\n",
    "    why = ['ทำไม']\n",
    "    who = ['ใคร']\n",
    "    whose = ['ของใคร']\n",
    "    which = ['อันไหน','หรือ']\n",
    "    what = ['อะไร','แบบไหน','ทำไร','ไหน']\n",
    "    how = ['อย่างไร','ยังไง','ทำไง','เท่าไหร่','เท่าไร','เป็นไง','มีกี่']\n",
    "    rec = ['แนะนำ','รีวิว']\n",
    "    yes_no = ['ใช่ไหม','ใช่มั้ย','ได้ไหม','หรือยัง','ได้ไหม','ไปไหม','ใช่ไหม','ได้มั้ย','มั้ย','หรือเปล่า','ไหม','ได้ป่าว','ได้ปะ','ได้รึเปล่า','มั๊ย','กันยัง']\n",
    "    \n",
    "    ques_types = []\n",
    "    \n",
    "    \n",
    "            \n",
    "    for w in where:\n",
    "        if w in text:\n",
    "            ques_types.append('where')\n",
    "            break\n",
    "            \n",
    "    for w in when:\n",
    "        if w in text:\n",
    "            ques_types.append('when')\n",
    "            break\n",
    "            \n",
    "    for w in why:\n",
    "        if w in text:\n",
    "            ques_types.append('why')\n",
    "            break\n",
    "            \n",
    "    for w in who:\n",
    "        if w in text:\n",
    "            ques_types.append('who')\n",
    "            break\n",
    "            \n",
    "    for w in whose:\n",
    "        if w in text:\n",
    "            ques_types.append('whose')\n",
    "            break\n",
    "            \n",
    "    for w in which:\n",
    "        if w in text:\n",
    "            ques_types.append('which')\n",
    "            break\n",
    "            \n",
    "    for w in what:\n",
    "        if w in text and 'which' not in ques_types and 'where' not in ques_types:\n",
    "            ques_types.append('what')\n",
    "            break\n",
    "            \n",
    "    for w in how:\n",
    "        if w in text:\n",
    "            ques_types.append('how')\n",
    "            break\n",
    "            \n",
    "    for w in rec:\n",
    "        if w in text:\n",
    "            ques_types.append('rec')\n",
    "            break\n",
    "            \n",
    "    for w in yes_no:\n",
    "        if w in text and 'when' not in ques_types:\n",
    "            ques_types.append('yes_no')\n",
    "            break\n",
    "    \n",
    "    return ques_types\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/predicted-non-questions - question.csv')\n",
    "df = df.drop(['label'],axis = 1)\n",
    "\n",
    "df2 = pd.read_csv('./data/predited-questions - question and _0.75.csv')\n",
    "result = df.append(df2,ignore_index=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_types(df):\n",
    "    data = []\n",
    "    for t in tqdm(df['text']):\n",
    "\n",
    "        types = ','.join(fine_ques_type(t))\n",
    "        \n",
    "        tokens = text_to_tokens(t)\n",
    "        topics = ','.join(pos_filter_noun(tokens))\n",
    "        \n",
    "        data.append({\n",
    "            'text':t,\n",
    "            'types':types,\n",
    "            'keywords':topics\n",
    "        })\n",
    "\n",
    "    df2 = pd.DataFrame(data)\n",
    "    return df2[['text','types','keywords']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = find_types(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
