{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pythainlp.util import normalize\n",
    "from pythainlp.ulmfit.utils import ThaiTokenizer\n",
    "from pythainlp.tag import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=numpy.nan)\n",
    "\n",
    "import re,string\n",
    "import pythainlp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tokens(text, stop_words=[]):\n",
    "    # nomalize\n",
    "    text = pythainlp.util.normalize(text)\n",
    "    \n",
    "    # remove duplicate ending characters\n",
    "    for m in re.finditer(r'([\\u0E00-\\u0E7F])(\\1{2,})', text):\n",
    "        text = text.replace(m.group(0),m.group(1),1)\n",
    "        \n",
    "    # remove url\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # remove CR\n",
    "    text = text.replace('CR', '').replace('SR', '').replace('\\xa0', '')\n",
    "        \n",
    "    # remove unwanted character\n",
    "    pattern = re.compile(r\"[^\\u0E00-\\u0E7Fa-zA-Z ]|ๆ\")\n",
    "    text = pattern.sub('',text)\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # tokenization\n",
    "    tokens = pythainlp.tokenize.word_tokenize(text, engine='newmm', whitespaces=False)\n",
    "    \n",
    "    # remove stopwords\n",
    "    tokens = [t for t in tokens if not t in stop_words] \n",
    "\n",
    "    return tokens\n",
    "\n",
    "def pos_filter_noun(tokens):\n",
    "    postags =  pos_tag(tokens, engine = 'artagger')\n",
    "    filterpos = []\n",
    "    noun = ['NPRP','NCMN']\n",
    "    for w in postags:\n",
    "        \n",
    "        # filter noun\n",
    "        if w[1] in noun:\n",
    "            filterpos.append(w[0])\n",
    "        \n",
    "    return filterpos\n",
    "\n",
    "def find_stopwords(df):\n",
    "    stopwords = []\n",
    "    \n",
    "    for index,row in df.iterrows():\n",
    "        if row['label'] == 0:\n",
    "            stopwords.append(row['Stopword'])\n",
    "    return stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Stopwords\n",
    "\n",
    "This part of code is used to create array of stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/stopwords.csv',encoding='utf-8-sig')\n",
    "df.fillna(0,inplace = True)\n",
    "\n",
    "stopwords = find_stopwords(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Corpus\n",
    "This part of code is used to create corpus which is a array of term and document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19356/19356 [00:09<00:00, 1974.48it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "# load second corpus \n",
    "df = pd.read_csv('./data/predicted-non-questions - question.csv',encoding='utf-8-sig')\n",
    "df = df.drop(['label'],axis = 1)\n",
    "\n",
    "# load first corpus\n",
    "df2 = pd.read_csv('./data/predited-questions - question and _0.75.csv',encoding='utf-8-sig')\n",
    "\n",
    "# merge both corpus\n",
    "all_question = df.append(df2,ignore_index=True)\n",
    "\n",
    "# loop tokenize each sentence in the corpus\n",
    "for w in tqdm(all_question['text']):\n",
    "    corpus.append(text_to_tokens(w,stopwords))\n",
    "    \n",
    "# join each tokenize in each sentence with \"<some_space>\"\n",
    "corpus = [\"<some_space>\".join(x) for x in corpus]\n",
    "\n",
    "# add tokenizer my own\n",
    "vectorizer = TfidfVectorizer(tokenizer = lambda x: x.split(\"<some_space>\"), analyzer=\"word\")\n",
    "\n",
    "# fit create a array of term and document.\n",
    "courpus_vector = vectorizer.fit_transform(corpus).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Find Similarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    \n",
    "    # find intersection\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    \n",
    "    # find union\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    \n",
    "    # find ratio\n",
    "    return float(intersection / union)\n",
    "\n",
    "# vectorizer is used to create a vector from a text, corpus vector is a array of terms and documents\n",
    "# thres is cosine similarity score, n is number of result, jac is jaccard_similarity score\n",
    "\n",
    "def find_sim_of_corpus(text,vectorizer,courpus_vector,thres = 0.5,n=10,jac = 0.1): \n",
    "    \n",
    "    # create the vector from the text\n",
    "    vector = \"<some_space>\".join(text_to_tokens(text))\n",
    "    vector = vectorizer.transform([vector]).toarray()[0]\n",
    "\n",
    "    # caculate score of the vector with the corpus\n",
    "    socres = []\n",
    "    i = 0\n",
    "    for s in courpus_vector:\n",
    "        score = cosine_similarity(s.reshape(1, -1),vector.reshape(1, -1))\n",
    "        socres.append({\n",
    "            \"text\":all_question['text'][i],\n",
    "            \"score\":score[0][0]\n",
    "        })\n",
    "        i+=1\n",
    "\n",
    "    df = pd.DataFrame(socres)\n",
    "    df = df.sort_values(by=['score'],ascending=False)\n",
    "    \n",
    "    # get keywords from the text\n",
    "    tokens_text = text_to_tokens(text)\n",
    "    topics_text = pos_filter_noun(tokens_text)\n",
    "    \n",
    "    # get top rank that fit with the setting condition\n",
    "    top = []\n",
    "    for index,row in tqdm(df.iterrows()):\n",
    "        \n",
    "        # get keywords from the sentence in the corpus\n",
    "        tokens = text_to_tokens(row['text'])\n",
    "        topics = pos_filter_noun(tokens)\n",
    "        \n",
    "        # find jaccard_similarity\n",
    "        jacard_score = jaccard_similarity(topics_text,topics)\n",
    "        \n",
    "        # check condition\n",
    "        if(row['score'] >= thres) and jacard_score >= jac:\n",
    "            top.append({\n",
    "                \"text\":row['text'],\n",
    "                \"score\":row['score'],\n",
    "                \"jacard\": jacard_score\n",
    "            })\n",
    "        \n",
    "        # check number of result\n",
    "        if len(top) == n:\n",
    "            break\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "find_sim_of_corpus() got multiple values for argument 'thres'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dfde7d03c25b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_sim_of_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ใครเคยไปเจาะเลือดที่จุฬาบ้าง'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcourpus_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresult_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: find_sim_of_corpus() got multiple values for argument 'thres'"
     ]
    }
   ],
   "source": [
    "sims = find_sim_of_corpus('ใครเคยไปเจาะเลือดที่จุฬาบ้าง',vectorizer,courpus_vector,thres= 0.1)\n",
    "result_df = pd.DataFrame(sims)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
