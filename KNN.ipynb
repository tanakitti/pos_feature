{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pythainlp.util import normalize\n",
    "from pythainlp.ulmfit.utils import ThaiTokenizer\n",
    "from pythainlp.tag import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=numpy.nan)\n",
    "\n",
    "import re,string\n",
    "import pythainlp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tokens(text, stop_words=[]):\n",
    "    # nomalize\n",
    "    text = pythainlp.util.normalize(text)\n",
    "    \n",
    "    # remove duplicate ending characters\n",
    "    for m in re.finditer(r'([\\u0E00-\\u0E7F])(\\1{2,})', text):\n",
    "        text = text.replace(m.group(0),m.group(1),1)\n",
    "        \n",
    "    # remove url\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # remove CR\n",
    "    text = text.replace('CR', '').replace('SR', '').replace('\\xa0', '')\n",
    "        \n",
    "    # remove unwanted character\n",
    "    pattern = re.compile(r\"[^\\u0E00-\\u0E7Fa-zA-Z ]|‡πÜ\")\n",
    "    text = pattern.sub('',text)\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # tokenization\n",
    "    tokens = pythainlp.tokenize.word_tokenize(text, engine='newmm', whitespaces=False)\n",
    "    \n",
    "    # remove stopwords\n",
    "    tokens = [t for t in tokens if not t in stop_words] \n",
    "\n",
    "    return tokens\n",
    "\n",
    "def pos_filter_noun(tokens):\n",
    "    postags =  pos_tag(tokens, engine = 'artagger')\n",
    "    filterpos = []\n",
    "    noun = ['NPRP','NCMN']\n",
    "    for w in postags:\n",
    "        \n",
    "        # filter noun\n",
    "        if w[1] in noun:\n",
    "            filterpos.append(w[0])\n",
    "        \n",
    "    return filterpos\n",
    "\n",
    "def find_stopwords(df):\n",
    "    stopwords = []\n",
    "    \n",
    "    for index,row in df.iterrows():\n",
    "        if row['label'] == 0:\n",
    "            stopwords.append(row['Stopword'])\n",
    "    return stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Stopwords\n",
    "\n",
    "This part of code is used to create array of stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/stopwords.csv',encoding='utf-8-sig')\n",
    "df.fillna(0,inplace = True)\n",
    "\n",
    "stopwords = find_stopwords(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Corpus\n",
    "This part of code is used to create corpus which is a array of term and document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19356/19356 [00:07<00:00, 2567.01it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "# load second corpus \n",
    "df = pd.read_csv('./data/predicted-non-questions - question.csv',encoding='utf-8-sig')\n",
    "df = df.drop(['label'],axis = 1)\n",
    "\n",
    "# load first corpus\n",
    "df2 = pd.read_csv('./data/predited-questions - question and _0.75.csv',encoding='utf-8-sig')\n",
    "\n",
    "# merge both corpus\n",
    "all_question = df.append(df2,ignore_index=True)\n",
    "\n",
    "# loop tokenize each sentence in the corpus\n",
    "for w in tqdm(all_question['text']):\n",
    "    corpus.append(text_to_tokens(w,stopwords))\n",
    "    \n",
    "# join each tokenize in each sentence with \"<some_space>\"\n",
    "corpus = [\"<some_space>\".join(x) for x in corpus]\n",
    "\n",
    "# add tokenizer my own\n",
    "vectorizer = TfidfVectorizer(tokenizer = lambda x: x.split(\"<some_space>\"), analyzer=\"word\")\n",
    "\n",
    "# fit create a array of term and document.\n",
    "courpus_vector = vectorizer.fit_transform(corpus).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Find Similarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    \n",
    "    # find intersection\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    \n",
    "    # find union\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    \n",
    "    # find ratio\n",
    "    return float(intersection / union)\n",
    "\n",
    "# vectorizer is used to create a vector from a text, corpus vector is a array of terms and documents\n",
    "# thres is cosine similarity score, n is number of result, jac is jaccard_similarity score\n",
    "\n",
    "def find_sim_of_corpus(text,vectorizer,courpus_vector,thres = 0.5,n=10,jac = 0.1): \n",
    "    \n",
    "    # create the vector from the text\n",
    "    vector = \"<some_space>\".join(text_to_tokens(text))\n",
    "    vector = vectorizer.transform([vector]).toarray()[0]\n",
    "\n",
    "    # caculate score of the vector with the corpus\n",
    "    socres = []\n",
    "    i = 0\n",
    "    for s in courpus_vector:\n",
    "        score = cosine_similarity(s.reshape(1, -1),vector.reshape(1, -1))\n",
    "        socres.append({\n",
    "            \"text\":all_question['text'][i],\n",
    "            \"score\":score[0][0]\n",
    "        })\n",
    "        i+=1\n",
    "\n",
    "    df = pd.DataFrame(socres)\n",
    "    df = df.sort_values(by=['score'],ascending=False)\n",
    "    \n",
    "    # get keywords from the text\n",
    "    tokens_text = text_to_tokens(text)\n",
    "    topics_text = pos_filter_noun(tokens_text)\n",
    "    \n",
    "    # get top rank that fit with the setting condition\n",
    "    top = []\n",
    "    for index,row in tqdm(df.iterrows()):\n",
    "        \n",
    "        # get keywords from the sentence in the corpus\n",
    "        tokens = text_to_tokens(row['text'])\n",
    "        topics = pos_filter_noun(tokens)\n",
    "        \n",
    "        # find jaccard_similarity\n",
    "        jacard_score = jaccard_similarity(topics_text,topics)\n",
    "        \n",
    "        # check condition\n",
    "        if(row['score'] >= thres) and jacard_score >= jac:\n",
    "            top.append({\n",
    "                \"text\":row['text'],\n",
    "                \"score\":row['score'],\n",
    "                \"jacard\": jacard_score\n",
    "            })\n",
    "        \n",
    "        # check number of result\n",
    "        if len(top) == n:\n",
    "            break\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 26.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jacard</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.782177</td>\n",
       "      <td>‡πÉ‡∏Ñ‡∏£‡πÑ‡∏õ‡πÄ‡∏à‡∏≤‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡∏°‡∏±‡πà‡∏á‡∏á‡∏á‡∏á</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.506322</td>\n",
       "      <td>‡∏°‡∏µ‡πÉ‡∏Ñ‡∏£‡∏ö‡πâ‡∏≤‡∏á‡∏°‡∏±‡πâ‡∏¢‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡∏ö‡πÄ‡∏à‡∏≤‡∏∞‡∏´‡∏π‡∏à‡∏¥‡∏ß‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡∏™‡∏≤‡∏î...‡∏≠‡πà‡∏≠‡∏Å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500940</td>\n",
       "      <td>‡∏à‡∏∏‡∏¨‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏á‡∏ö‡πâ‡∏≤‡∏á?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.457750</td>\n",
       "      <td>‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏∏‡∏¨‡∏≤‡∏ö‡πâ‡∏≤‡∏á‡∏°‡∏±‡∏ô‡∏¢‡∏≤‡∏Å‡πÑ‡∏´‡∏°‡∏°‡∏°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.373428</td>\n",
       "      <td>‡πÉ‡∏Ñ‡∏£‡∏°‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÉ‡∏ô‡∏à‡∏∏‡∏¨‡∏≤‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ö‡πâ‡∏≤‡∏á</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.363304</td>\n",
       "      <td>‡∏≠‡∏¢‡∏≤‡∏Å‡∏°‡∏µ‡πÅ‡∏ü‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏∏‡∏¨‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÑ‡∏á‡∏≠‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏î‡πá‡∏Å‡∏à‡∏∏‡∏¨‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡∏ö...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.344686</td>\n",
       "      <td>‡∏ó‡∏≥‡πÑ‡∏á‡πÉ‡∏´‡πâ‡∏á‡πà‡∏ß‡∏á‡πÄ‡∏£‡πá‡∏ß‡πÜ‡∏ô‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏õ‡∏ö‡∏£‡∏¥‡∏à‡∏≤‡∏Ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏îüè©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.338383</td>\n",
       "      <td>‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏∏‡∏¨‡∏≤‡πÅ‡∏ï‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏∏‡∏¨‡∏≤‡∏•‡∏á‡∏Å‡∏•‡∏≠‡∏ô</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.335696</td>\n",
       "      <td>‡∏ó‡∏≥‡πÑ‡∏°‡πÄ‡∏£‡∏≤‡∏ñ‡∏∂‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏∏‡∏¨‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333829</td>\n",
       "      <td>‡∏≠‡∏¢‡∏≤‡∏Å‡∏°‡∏µ‡πÅ‡∏ü‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏∏‡∏¨‡∏≤5555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     jacard     score                                               text\n",
       "0  0.333333  0.782177                              ‡πÉ‡∏Ñ‡∏£‡πÑ‡∏õ‡πÄ‡∏à‡∏≤‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡∏°‡∏±‡πà‡∏á‡∏á‡∏á‡∏á\n",
       "1  0.111111  0.506322   ‡∏°‡∏µ‡πÉ‡∏Ñ‡∏£‡∏ö‡πâ‡∏≤‡∏á‡∏°‡∏±‡πâ‡∏¢‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡∏ö‡πÄ‡∏à‡∏≤‡∏∞‡∏´‡∏π‡∏à‡∏¥‡∏ß‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡∏™‡∏≤‡∏î...‡∏≠‡πà‡∏≠‡∏Å\n",
       "2  0.333333  0.500940                                    ‡∏à‡∏∏‡∏¨‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏á‡∏ö‡πâ‡∏≤‡∏á?\n",
       "3  0.333333  0.457750                        ‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏∏‡∏¨‡∏≤‡∏ö‡πâ‡∏≤‡∏á‡∏°‡∏±‡∏ô‡∏¢‡∏≤‡∏Å‡πÑ‡∏´‡∏°‡∏°‡∏°\n",
       "4  0.500000  0.373428                     ‡πÉ‡∏Ñ‡∏£‡∏°‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÉ‡∏ô‡∏à‡∏∏‡∏¨‡∏≤‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ö‡πâ‡∏≤‡∏á\n",
       "5  0.111111  0.363304  ‡∏≠‡∏¢‡∏≤‡∏Å‡∏°‡∏µ‡πÅ‡∏ü‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏∏‡∏¨‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÑ‡∏á‡∏≠‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏î‡πá‡∏Å‡∏à‡∏∏‡∏¨‡∏≤‡∏ä‡πà‡∏ß‡∏¢‡∏ö...\n",
       "6  0.250000  0.344686                ‡∏ó‡∏≥‡πÑ‡∏á‡πÉ‡∏´‡πâ‡∏á‡πà‡∏ß‡∏á‡πÄ‡∏£‡πá‡∏ß‡πÜ‡∏ô‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏õ‡∏ö‡∏£‡∏¥‡∏à‡∏≤‡∏Ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏îüè©\n",
       "7  0.250000  0.338383          ‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏∏‡∏¨‡∏≤‡πÅ‡∏ï‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏à‡∏∏‡∏¨‡∏≤‡∏•‡∏á‡∏Å‡∏•‡∏≠‡∏ô\n",
       "8  0.333333  0.335696                           ‡∏ó‡∏≥‡πÑ‡∏°‡πÄ‡∏£‡∏≤‡∏ñ‡∏∂‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏à‡∏∏‡∏¨‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ\n",
       "9  0.333333  0.333829                             ‡∏≠‡∏¢‡∏≤‡∏Å‡∏°‡∏µ‡πÅ‡∏ü‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏∏‡∏¨‡∏≤5555"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = find_sim_of_corpus('‡πÉ‡∏Ñ‡∏£‡πÄ‡∏Ñ‡∏¢‡πÑ‡∏õ‡πÄ‡∏à‡∏≤‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏¨‡∏≤‡∏ö‡πâ‡∏≤‡∏á',vectorizer,courpus_vector,thres= 0.1)\n",
    "result_df = pd.DataFrame(sims)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
